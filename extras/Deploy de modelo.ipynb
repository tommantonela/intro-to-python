{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xrsOVS_2Iyez"
   },
   "source": [
    "# **Deploy de un modelo**\n",
    "\n",
    "En esta notebook vamos a jugar con la creación de un modelo basados en el procesamiento que hicimos en notebooks pasadas y vamos a hacer un mini deploy de dicho modelo.\n",
    "\n",
    "*Nota*. Para los efectos de esta notebook vamos a usar una estrategia simple de división del dataset en training-test. Se podrían optar por otras opciones u otros esquemas de división. Asimismo, si bien se muestra una posibilidad para la elección del \"mejor\" modelo mediante una optimización de parámetros, no será utilizada para el modelo a guardar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3udATCXHI7Zl"
   },
   "source": [
    "### Creación del modelo\n",
    "\n",
    "Primero, vamos a traernos los datos con los que vamos a estar trabajando. Vamos a seguir utilizando el dataset simple de detección de hate speech, del que nos vamos a quedar con un atributo de tipo texto y la clase numérica (0: No es hate speech, 1: hate speech, 2: offensive speech).\n",
    "\n",
    "*Nota:* No se preocupen mucho por el procesamiento de texto que hacemos en este ejemplo, ya lo veremos más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2214,
     "status": "ok",
     "timestamp": 1597512197182,
     "user": {
      "displayName": "Antonela Tommasel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZHZvwAe7IBUffT-rd4hTz_uFFKkvW9vlfJO87sg=s64",
      "userId": "05608722526508464314"
     },
     "user_tz": 180
    },
    "id": "ptD3Rd0jItpM",
    "outputId": "2b598031-b24f-4118-e483-2c8d6523bce6"
   },
   "outputs": [],
   "source": [
    "# Cargamos los datos necesarios\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/t-davidson/hate-speech-and-offensive-language/master/data/labeled_data.csv\"\n",
    "df = pd.read_csv(url, usecols=['class', 'tweet']) # de todas las columnas que tiene el dataset, nos vamos a quedar solo con el texto y la clase\n",
    "\n",
    "print(df[:1000]) # limitamos la cantidad de instancias para que no tarde ni el pre-processing ni el training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 771,
     "status": "ok",
     "timestamp": 1597512227405,
     "user": {
      "displayName": "Antonela Tommasel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZHZvwAe7IBUffT-rd4hTz_uFFKkvW9vlfJO87sg=s64",
      "userId": "05608722526508464314"
     },
     "user_tz": 180
    },
    "id": "LGR3bqdtKH-0"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(df, test_size = 0.80,random_state=42) # limitamos el tamaño del training para que no tarde\n",
    "\n",
    "# recordemos que para entrenar tenemos separar la clase\n",
    "X_train = train_set.drop(\"class\", axis=1)  \n",
    "y_train = train_set[\"class\"].copy()\n",
    "\n",
    "X_test = test_set.drop(\"class\",axis=1) # nos dejamos también preparado el test set\n",
    "y_test = test_set[\"class\"].copy() # nos dejamos también preparado el test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1434,
     "status": "ok",
     "timestamp": 1597512229836,
     "user": {
      "displayName": "Antonela Tommasel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZHZvwAe7IBUffT-rd4hTz_uFFKkvW9vlfJO87sg=s64",
      "userId": "05608722526508464314"
     },
     "user_tz": 180
    },
    "id": "iwaiAEJ7K4Gp"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 739,
     "status": "ok",
     "timestamp": 1597512231563,
     "user": {
      "displayName": "Antonela Tommasel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZHZvwAe7IBUffT-rd4hTz_uFFKkvW9vlfJO87sg=s64",
      "userId": "05608722526508464314"
     },
     "user_tz": 180
    },
    "id": "eWUEAR0-LQtv",
    "outputId": "7dbcf60a-40cc-4409-dec7-aacc30725cd1"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 667,
     "status": "ok",
     "timestamp": 1597512251376,
     "user": {
      "displayName": "Antonela Tommasel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZHZvwAe7IBUffT-rd4hTz_uFFKkvW9vlfJO87sg=s64",
      "userId": "05608722526508464314"
     },
     "user_tz": 180
    },
    "id": "Mj3slMA7LCL2"
   },
   "outputs": [],
   "source": [
    "count_normal = CountVectorizer(stop_words=nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('count', count_normal, \"tweet\")]) # importante definir las columnas sobre las cuales se aplica\n",
    "\n",
    "rf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', SVC(probability=True))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4FuOff5LLgRE"
   },
   "source": [
    "Finalmente, vamos a entrenar el modelo. De acuerdo al modelo que elijamos, puede tardar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17286,
     "status": "ok",
     "timestamp": 1597512270004,
     "user": {
      "displayName": "Antonela Tommasel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZHZvwAe7IBUffT-rd4hTz_uFFKkvW9vlfJO87sg=s64",
      "userId": "05608722526508464314"
     },
     "user_tz": 180
    },
    "id": "Zgo2aIF6LLee",
    "outputId": "be7f69d8-fe04-4c6c-f567-4c538f46a5e6"
   },
   "outputs": [],
   "source": [
    "rf.fit(X_train,y_train)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8313,
     "status": "ok",
     "timestamp": 1597512283321,
     "user": {
      "displayName": "Antonela Tommasel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZHZvwAe7IBUffT-rd4hTz_uFFKkvW9vlfJO87sg=s64",
      "userId": "05608722526508464314"
     },
     "user_tz": 180
    },
    "id": "gUDt1xGLL5FK",
    "outputId": "41b8c072-dfbb-44d0-b3cf-9b69a415a522"
   },
   "outputs": [],
   "source": [
    "rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uL7CrzKiUG76"
   },
   "source": [
    "### Model selection\n",
    "\n",
    "El pipeline que definimos también puede ser utilizado en el proceso de selección de modelos. En el siguiente fragmento de código se cicla por diferentes modelos de clasificación provistos por sklearn, para aplicar las transformaciones y luego entrenarlos.\n",
    "\n",
    "Nota. Hay más clasificadores disponibles para probar.\n",
    "\n",
    "Nota 2. Puede tardar!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20679,
     "status": "ok",
     "timestamp": 1597512333140,
     "user": {
      "displayName": "Antonela Tommasel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZHZvwAe7IBUffT-rd4hTz_uFFKkvW9vlfJO87sg=s64",
      "userId": "05608722526508464314"
     },
     "user_tz": 180
    },
    "id": "hwUloUnJUHMn",
    "outputId": "6ac46c8d-0267-46a1-f50c-035c65596d20"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    SVC()\n",
    "    ]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', classifier)])\n",
    "    pipe.fit(X_train, y_train)   \n",
    "    print(classifier)\n",
    "    print(\"model score: %.3f\" % pipe.score(X_test, y_test)) # qué retorna depende del modelo que se usa. En clasificación retorna accuracy promedio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCK8dwi0UKxM"
   },
   "source": [
    "Finalmente, el pipeline que definimos también puede ser utilizado en un grid search para encontrar la mejor combinación de hiper-parámetros.\n",
    "\n",
    "Para hacer esto, lo primero que hay que hacer es crear una grilla de parámetros para el modelo elegido. Algo importante a notar es que a los nombres de los parámetros hay que agregarles el nombre que le dimos al parámetro que representaba al algoritmo (en este caso de clasificación, al que llamamos ``classifier``).\n",
    "\n",
    "Luego, creamos el objeto de grid search el cual incluye el pipeline original. Cuando llamemos al método ``fit``, antes de realizar la búsqueda del grid search se aplicarán las transformaciones.\n",
    "\n",
    "Nota. En este ejemplo se están considerando dos parámetros para el ``RandomForestClassifier``. De acuerdo al clasificador, los parámetros que se podrán optimizar.\n",
    "\n",
    "Nota 2. Hay múltiples métricas de [scoring](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring) que pueden ser consideradas. Ver también la documentación referida a [model evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html).\n",
    "\n",
    "Nota 3. Puede tardar!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6727,
     "status": "ok",
     "timestamp": 1597512389269,
     "user": {
      "displayName": "Antonela Tommasel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZHZvwAe7IBUffT-rd4hTz_uFFKkvW9vlfJO87sg=s64",
      "userId": "05608722526508464314"
     },
     "user_tz": 180
    },
    "id": "OPeqlKYpUMgA",
    "outputId": "46c0af00-3c4c-42cc-efcd-f039c2ad1e28"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rfcv = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', RandomForestClassifier())])\n",
    "\n",
    "param_grid = { \n",
    "    'classifier__n_estimators': [1, 3, 10],\n",
    "    'classifier__max_features': ['auto', 'sqrt'],\n",
    "}\n",
    "\n",
    "CV = GridSearchCV(rfcv, param_grid, cv=5,\n",
    "                           scoring='f1_weighted') \n",
    "                  \n",
    "CV.fit(X_train, y_train)  \n",
    "print(CV.best_params_)    \n",
    "print(CV.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gFprFbx6KHyo"
   },
   "source": [
    "### Salvar/Exportar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xTL5idLxKHKP"
   },
   "source": [
    "En este caso el modelo lo necesitamos solo acá, pero qué pasaría si nosotros quisieramos llevar este modelo a otro ambiente o simplemente reemplazar otro modelo que teníamos por este. Tenemos que repetir todos los pasos de definición del pipeline y reentrenar? No, no es necesario.\n",
    "\n",
    "Lo que podemos hacer es persistir el modelo, es decir, guardarlo en un archivo que luego podremos levantar en donde nosotros quisiéramos utilizarlo.\n",
    "\n",
    "La primera alternativa es usar ```joblib```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1041,
     "status": "ok",
     "timestamp": 1597512712080,
     "user": {
      "displayName": "Antonela Tommasel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZHZvwAe7IBUffT-rd4hTz_uFFKkvW9vlfJO87sg=s64",
      "userId": "05608722526508464314"
     },
     "user_tz": 180
    },
    "id": "TySSFiv2KKJr",
    "outputId": "1b33e618-928d-4582-e186-8a8e28d52ccd"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(rf, \"hate_speech_detection_model.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LI1xK_6sKSRq"
   },
   "source": [
    "Luego, para cargarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8551,
     "status": "ok",
     "timestamp": 1597512733391,
     "user": {
      "displayName": "Antonela Tommasel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZHZvwAe7IBUffT-rd4hTz_uFFKkvW9vlfJO87sg=s64",
      "userId": "05608722526508464314"
     },
     "user_tz": 180
    },
    "id": "TzmDJOmDKMg3",
    "outputId": "6a3d71e3-6769-406c-dcd4-09149ac0c90b"
   },
   "outputs": [],
   "source": [
    "loaded_model = joblib.load(\"hate_speech_detection_model.pkl\")\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0m9eHowwKRHe"
   },
   "source": [
    "Otra alternativa es usar ```Pickle```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 885,
     "status": "ok",
     "timestamp": 1597512739220,
     "user": {
      "displayName": "Antonela Tommasel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZHZvwAe7IBUffT-rd4hTz_uFFKkvW9vlfJO87sg=s64",
      "userId": "05608722526508464314"
     },
     "user_tz": 180
    },
    "id": "g8JY9pdyKOVl"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(rf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SntUNJmXKWke"
   },
   "source": [
    "Luego, para cargarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8466,
     "status": "ok",
     "timestamp": 1597512751395,
     "user": {
      "displayName": "Antonela Tommasel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZHZvwAe7IBUffT-rd4hTz_uFFKkvW9vlfJO87sg=s64",
      "userId": "05608722526508464314"
     },
     "user_tz": 180
    },
    "id": "yD-WapI6KPam",
    "outputId": "e9ab7dfd-dc89-437c-b53f-e56110a5eda5"
   },
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_yeR9K9jKq-c"
   },
   "source": [
    "### Deploy del modelo\n",
    "\n",
    "Para hacer el deploy del modelo, vamos a crear una aplicación de hate speech detection para deployarla como un servicio REST básandonos en el modelo que creamos en los bloques anteriores.\n",
    "\n",
    "Vamos a usar:\n",
    "\n",
    "* [Flask](https://github.com/pallets/flask): uno de los micro web frameworks más populares.\n",
    "* [flask_ngrok](https://pypi.org/project/flask-ngrok/): herramienta que nos permite hacer demos de nuestras apps Flask. Nos permite servir nuestra applicación desde una simple notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XChYQegAMfx2"
   },
   "source": [
    "Instalamos las dependencias que vamos a necesitar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9048,
     "status": "ok",
     "timestamp": 1597512767740,
     "user": {
      "displayName": "Antonela Tommasel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZHZvwAe7IBUffT-rd4hTz_uFFKkvW9vlfJO87sg=s64",
      "userId": "05608722526508464314"
     },
     "user_tz": 180
    },
    "id": "Q-xecug8Mbts",
    "outputId": "e108eb4d-8132-4bd9-f3bd-fb50da000d9b"
   },
   "outputs": [],
   "source": [
    "!pip install flask\n",
    "!pip install flask-ngrok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kgMmPHfyMnKI"
   },
   "source": [
    "Lo primero que vamos a hacer es dejar accessible nuestro modelo entrenado a nuestra aplicación. \n",
    "\n",
    "En este caso, si venimos ejecutando toda la notebook vamos a tener disponible nuestro ``hate_speech_detection_model.pkl``.\n",
    "\n",
    "Nota. Cada vez que reiniciemos el runtime, deberíamos generar o levantar nuestro modelo de algún lado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 955
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 103711,
     "status": "ok",
     "timestamp": 1597512911953,
     "user": {
      "displayName": "Antonela Tommasel",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZHZvwAe7IBUffT-rd4hTz_uFFKkvW9vlfJO87sg=s64",
      "userId": "05608722526508464314"
     },
     "user_tz": 180
    },
    "id": "x5HqKr94Mmtp",
    "outputId": "24fd2c7d-6575-46b9-d0ac-6da7181ac3ce"
   },
   "outputs": [],
   "source": [
    "from flask_ngrok import run_with_ngrok\n",
    "from flask import Flask,request,jsonify\n",
    "import pandas as pd \n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "run_with_ngrok(app)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "  return \"<h1>Hate Speech Detector!</h1>\"\n",
    "\n",
    "model = joblib.load(\"hate_speech_detection_model.pkl\") # vamos a levantar nuestro modelo\n",
    "\n",
    "@app.route('/predict',methods=['GET','POST']) # los tipos de métodos que soportamos\n",
    "def predict():\n",
    "  \n",
    "  df_n = pd.DataFrame({\"tweet\":[request.args['text']]}) # tomamos el texto que nos pasaron para hacer la predicción\n",
    "\n",
    "  prediction = model.predict(df_n) # utilizamos el modelo para predecir\n",
    "  pred_proba = model.predict_proba(df_n) # obtenemos la probabilidad de la predicción --> No está disponible para todos los clasificadores!\n",
    "\n",
    "  if prediction == 0:\n",
    "    pred_text = 'Hate'\n",
    "  elif prediction == 1:\n",
    "    pred_text = 'Offensive'\n",
    "  else:\n",
    "    pred_text = 'Neither'   \n",
    "\n",
    "  print(request.args['text']) # en la consola de acá imprimimos el texto\n",
    "  print(pred_proba.dtype) # en la consola de acá imprimimos la probabilidad\n",
    "\n",
    "  return \"<h2>El texto \\\"\"+request.args['text']+\"\\\" fue clasificado como: \"+pred_text+\" con una probabilidad de: \"+str(pred_proba[0][prediction])+\"</h2>\"\n",
    "\n",
    "app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qu2SCKUZMrq4"
   },
   "source": [
    "Y listo! Ahora ya tenemos nuestro detector de hate speech disponible! Si le pasamos un argumento ```text``` con un string nos va a retornar si es o no hate speech! Para pasarle el argumento ``URL_NGROK/predict?text=TEXTO``. Por ejemplo, ``http://b36fd66da979.ngrok.io/predict?text=\"I hate everyone!!\"``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8gn3VOBHMtQI"
   },
   "source": [
    "El formato de salida que le dimos no es muy amigable con el usuario si lo que queremos es dejarlo disponible y que otros lo usen. Para eso, vamos a modificar la salida para que nos retorne un json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 831
    },
    "colab_type": "code",
    "id": "o1awUhRZMqvX",
    "outputId": "3f8ea805-5cbd-4dd3-fb27-d556a6d82290"
   },
   "outputs": [],
   "source": [
    "# es el mismo código que antes, solo cambia el return\n",
    "\n",
    "from flask_ngrok import run_with_ngrok\n",
    "from flask import Flask,request,jsonify\n",
    "import pandas as pd \n",
    "from sklearn.externals import joblib\n",
    "import json\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "run_with_ngrok(app)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "  return \"<h1>Hate Speech Detector!</h1>\"\n",
    "\n",
    "model = joblib.load(\"hate_speech_detection_model.pkl\") \n",
    "\n",
    "@app.route('/predict',methods=['GET','POST'])\n",
    "def predict():\n",
    "  \n",
    "  df_n = pd.DataFrame({\"tweet\":[request.args['text']]})\n",
    "\n",
    "  prediction = model.predict(df_n)\n",
    "  pred_proba = model.predict_proba(df_n)\n",
    "\n",
    "  if prediction == 0:\n",
    "    pred_text = 'Hate'\n",
    "  elif prediction == 1:\n",
    "    pred_text = 'Offensive'\n",
    "  else:\n",
    "    pred_text = 'Neither'  \n",
    "\n",
    "  output = {'text': request.args['text'], 'prediction': pred_text, 'confidence': str(pred_proba[0][prediction])}\n",
    "\n",
    "  return json.dumps(output)\n",
    "\n",
    "app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zObccEldMw6-"
   },
   "source": [
    "Si queremos probar de consumirlo como un servicio \"normal\", podemos ejecutar el siguiente código (en otra notebook, dado que acá estamos ejecutando el server)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dtVYH7KRMy9S"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "text = \"I think you are not pretty\"\n",
    "base = \"COMPLETAR_URL_NGROK\"\n",
    "url = base+'/predict?text='+text\n",
    "\n",
    "response = requests.post(url)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JJwdLAfZM1yl"
   },
   "source": [
    "Otra posibilidad sería esto mismo deployarlo en algún proveedor Cloud o incluso hacerlo accesible como una imagen de Docker.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOtwEhFzMeIrDtYLJ6Qja9d",
   "collapsed_sections": [],
   "name": "DUIA NLP (C) - Deploy de modelo",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
