{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qsk1NtwQcsUK"
   },
   "source": [
    "## **Pipelines & Transformers**\n",
    "\n",
    "En esta notebook vamos a jugar con la creación de Pipelines y Transformers/Vectorizers para simplificar el desarrollo de un pipeline de procesamiento, que incluya desde el pre-procesamiento de los datos, hasta el entrenamiento y testing de un clasificador.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJFtZCmvzA-h"
   },
   "source": [
    "Como venimos trabajando, en la mayoría de las aplicaciones de Machine Learning, los datos con los que hay que trabajar no se encuentran en las condiciones óptimas para entrenar el \"mejor\" modelo posible.\n",
    "\n",
    "Existen diversos pasos a realizar a las features, dependientes su tipo. Por ejemplo, puede haber un encoding para las features nominales o categóricas, escalado y normalización para features numéricas, y otras tantas alternativas de pre-procesamiento para el texto. Recordemos que los atributos textuales no son bien recibidos por los modelos a entrenar dado que los mismos esperan una representación numérica.\n",
    "\n",
    "En las notebooks anteriores el procesamiento lo realizamos de forma \"manual\", es decir, creando algunos métodos para aplicar el pre-procesamiento a los datos de forma independiente de otras estructuras o tareas. Sin embargo, esta sepración e independencia puede resultar \"tediosa\". Si el objetivo último es entrenar un modelo, debemos recordar que hay que aplicar el pre-procesamiento tanto al training como al test, y luego a cada uno de los elementos que querramos evaluar con el modelo. Por otra parte, si quisieramos compartir nuestro modelo con otros, también tenemos que recordar compartir el pre-procesamiento de los datos aparte.\n",
    "\n",
    "En este contexto, ``Scikit-learn`` nos provee de algunos mecanismos para simplificar la integración de las tareas y el proceso: los pipelines y los transformers, que permiten:\n",
    "\n",
    "* Hacer que el workflow sea más fácil de leer y entender.\n",
    "* Mejorar la \"organización\" del workflow.\n",
    "* Incrementar la reproducibilidad del proceso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMVsMyp9fOoz"
   },
   "source": [
    "#### Algunas definiciones\n",
    "\n",
    "##### ``fit`` vs ``transform``\n",
    "\n",
    "* ``fit`` encuentra los parámetros del modelo que será luego utilizado para transformar los datos. No necesariamente tiene que hacer algo más que retornar el objeto ``Transformer``, es decir, si mismo. \n",
    "\n",
    "* ``transform`` aplica la transformación a los datos de entrada, retornando los datos transformados.\n",
    "\n",
    "* ``fit_transform`` aplica el ``fit`` y ``transform`` de forma consecutiva.\n",
    "\n",
    "Cuando estamos usando un ``Pipeline``, ``fit`` y ``fit_transform`` tienen el mismo comportamiento salvo para el último elemento del pipeline. Para aquellos elementos previos al último realizan el ``fit`` de cada uno de los elementos del pipe y luego sus correspondientes ``transform``, mientras que para el último, el ``fit`` invoca al ``fit`` y el ``fit_transform`` al ``fit_transform``. \n",
    "\n",
    "En el caso de los estimadores, también tendremos el ``predict`` que aplica los ``transform`` sobre los datos y luego realiza el ``predict`` sobre el último elemento del pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3_b25NR0jDy"
   },
   "source": [
    "Para este ejemplo, vamos a utilizar un dataset simple de tweets para la detección de *hate speech*, del que nos vamos a quedar con un atributo de tipo texto y la clase numérica (0: No es hate speech, 1: hate speech, 2: offensive speech)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pplPeZ7U0imp"
   },
   "outputs": [],
   "source": [
    "# Cargamos los datos necesarios\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/t-davidson/hate-speech-and-offensive-language/master/data/labeled_data.csv\"\n",
    "df = pd.read_csv(url, usecols=['class', 'tweet']) # de todas las columnas que tiene el dataset, nos vamos a quedar solo con el texto y la clase\n",
    "df = df[60:65] # limitamos la cantidad de filas del dataframe\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GxZEgP6cjvyy"
   },
   "outputs": [],
   "source": [
    "df[df['class']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72zxfN4J04u0"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "def posStats (textSample):\n",
    "    # Función para contar las parts of speech\n",
    "    # https://stackoverflow.com/questions/10674832/count-verbs-nouns-and-other-parts-of-speech-with-pythons-nltk\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    # print(str(textSample))\n",
    "    tokens = nltk.word_tokenize(str(textSample))\n",
    "    print(len(tokens))\n",
    "    # print(tokens)\n",
    "    text = nltk.Text(tokens)\n",
    "    # print(len(text))\n",
    "    tags = nltk.pos_tag(text)\n",
    "    # print(len(tags))\n",
    "\n",
    "    counts = Counter(tag for word,tag in tags)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6VwSL811aUD"
   },
   "outputs": [],
   "source": [
    "print(posStats(df['tweet']))\n",
    "print(posStats(df['tweet'].str.cat(sep=' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIiGkTB01a2I"
   },
   "source": [
    "Como el objetivo último de este transformer es combinarlo con un clasificador, vamos a crear un split de training y test para poder probar y ver el resultado de aplicar el ``Transformer`` y de integrarlo en el pipeline completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3eVIoQ81ZQ8"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(df, test_size = 0.20,random_state=42)\n",
    "\n",
    "# recordemos que para entrenar tenemos separar la clase\n",
    "X_train = train_set.drop(\"class\", axis=1)  \n",
    "y_train = train_set[\"class\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeSyI0It1Dfp"
   },
   "source": [
    "### Opción 1: Agregamos pre-procesamiento a los Transformers que ya existen en sklearn\n",
    "\n",
    "Ya vienen incluidos algunos transformers para aplicar a atributos textuales. En este caso, vamos a tomar como base ``CountVectorizer``. Este vectorizer permite definir algunas configuraciones, como por ejemplo: (copiado de la [documentación](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html))\n",
    "\n",
    "* ``strip_accents : {‘ascii’, ‘unicode’}, default=None``.\n",
    "Eliminar acentos y otras normalizaciones. ``ascii`` es más rápida, pero solo funciona con chars que tienen un mapping directo a ascii. ``unicode`` un poco más lento, pero funciona sobre todos los chars. Utilizan la normalización NFKD.\n",
    "\n",
    "* ``lowercase bool, default=True``\n",
    "Convierte todos los chars a minúscula antes de tokenizar.\n",
    "\n",
    "* ``analyzer : {‘word’, ‘char’, ‘char_wb’} or callable, default=’word’``\n",
    "Si se debe analizar a nivel palabra o a nivel n-chars. ``char_wb`` saca los n-chars de adentro de las palabras y lo que \"sobre\" es completado con espacios en blanco.\n",
    "\n",
    "* ``stop_words : {‘english’}, list, default=None``\n",
    "Solo se utiliza a nivel ``word`` de análisis. Se le puede pasar una lista de stopwords. \n",
    "\n",
    "* ``ngram_range : tuple (min_n, max_n), default=(1, 1)``\n",
    "Tamaño de los n-grams a seleccionar. Se selecciona ``min_n <= n <= max_n``, por defecto se seleccionan palabras individuales (1-gram).\n",
    "\n",
    "* ``min_df : float or int, default=1``\n",
    "Setear la mínima frecuencia de documentos de una característica para ser incluido. Si se utiliza un float en el rango ``[0.0, 1.0]`` se indica la proporción de docuemntos.\n",
    "\n",
    "* ``max_df : float or int, default=1.0``\n",
    "Ignorar las característica que tienen una frecuencia a la definida. Si se utiliza un float en el rango ``[0.0, 1.0]`` se indica la proporción de docuemntos. De acuerdo con la documentación se puede utilizar en reemplazo de los stopwords si se setea en el rango ``[0.7, 1.0)``.\n",
    "\n",
    "* ``max_features : int, default=None``\n",
    "Solo incluir las características en el top-N ordenadas por su frecuencia.\n",
    "\n",
    "* ``preprocessor : callable, default=None``\n",
    "Sobre-escribir el pre-procesamiento pero mantener el tokenizer y la generación de n-grams.\n",
    "\n",
    "* ``tokenizer : callable, default=None``\n",
    "Sobre-escribir el paso de tokenización pero mantener el pre-procesamiento y la generación de n-grams.\n",
    "\n",
    "Como se puede ver, la mayoría de los pasos que vimos de pre-procesamiento pueden ser configurados en el ``Vectorizer``. Sin embargo, las posibilidades están restringidas a las implementaciones de las mencionadas basadas en NLTK. Cualquier otra cosa por fuera de NLTK debemos implementarla nosotros, ya sea como parte del ``Vectorizer`` o previa a la transformación. Por ejemplo, agregar corrector ortográfico. \n",
    "\n",
    "En esta notebook vamos a incorporar el procesamiento a los ``Vectorizers``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myY56psJKiAP"
   },
   "source": [
    "Si miramos la descripción de los diferentes parámetros que acepta el ``Vectorizer``, vamos a ver que ``preprocesor`` acepta un ``callable``, lo que significa que le podemos pasar un método para que sea ejecutado. Lo importante a considerar es que el método se ejecutará sobre cada **token**, no sobre el texto completo que reciba.\n",
    "\n",
    "Vamos a implementar un método que aplique corrección ortográfica a los tokens que encuentre. Ese método lo vamos a usar como parámetro del ``preprocessor``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ivuBTkIiLsRZ"
   },
   "outputs": [],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gAD3vkYfMJZC"
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VWT3QN4MImL"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3BajSvEF1C_u"
   },
   "outputs": [],
   "source": [
    "def preprocess(s): \n",
    "  return str(TextBlob(s).correct()).lower()\n",
    "\n",
    "count_transformer = CountVectorizer(stop_words=nltk.corpus.stopwords.words('english'),preprocessor=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BRZfltW9L4L7"
   },
   "outputs": [],
   "source": [
    "transformed_tweets = count_transformer.fit_transform(df[\"tweet\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plquMbZGObaq"
   },
   "source": [
    "Depende de la definición de los stopwords, puede generarse algún conflicto por el orden en el que se ejecutan los procesamientos. Fijense que si sacamos el ``lower()`` en nuestro procesamiento, los tokens no son convertidos a ``lowercase`` aun cuando el valor por default para ``lowercase`` es ``True``.\n",
    "\n",
    "Veamos la diferencia del conjunto de características que obtenemos si no aplicamos nuestro ``preprocessor``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_jPGab2OYOw"
   },
   "outputs": [],
   "source": [
    "print(len(count_transformer.get_feature_names()))\n",
    "print(sorted(count_transformer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5034IZwHNwIx"
   },
   "outputs": [],
   "source": [
    "count_normal = CountVectorizer(stop_words=nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "transformed_tweets_normal = count_normal.fit_transform(df[\"tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQJsrLE_Oi06"
   },
   "outputs": [],
   "source": [
    "print(len(count_normal.get_feature_names()))\n",
    "print(sorted(count_normal.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzfLHe79P8IA"
   },
   "source": [
    "Ahora, vamos a probar de usar nuestro ``Vectorizer`` en un pipeline completo. Para eso, vamos a definir un ``ColumnTransformer``, al que le vamos a agregar nuestro transformer, indicando la columna sobre la cual aplicarlo. Luego, agregamos al pipeline también el modelo elegido para entrenar y entrenamos!\n",
    "\n",
    "Esto nos permite combinar el pre-procesamiento con el entrenamiento del modelo y de forma similar también vamos a poder hacer la evaluación del modelo. Notar que al incorporar el procesamiento al pipeline, solo tenemos que darle la estructura original que soporta nuestros tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DhpGEfcQTMR"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHA5l9xQ2V4F"
   },
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('count', count_transformer, \"tweet\")]) # importante definir las columnas sobre las cuales se aplica\n",
    "\n",
    "rf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression())])\n",
    "\n",
    "rf.fit(X_train,y_train)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4P2cZLJpdK5U"
   },
   "source": [
    "En este pipeline implementamos un método para el ``preprocessor``, ahora vamos a modificar el ``tokenizer``. La diferencia con el anterior, es que se aplica sobre **todo el texto**.\n",
    "\n",
    "Para este ejemplo, vamos a crear un tokenizer que nos reemplace los tokens que aparecen en el texto por su etiqueta POS, basándonos en spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Kkh_ou7d_0y"
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bEHUr1Wxd3ue"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenizer(sent):\n",
    "  toks = []\n",
    "  for token in nlp(sent):\n",
    "    toks.append(token.tag_)\n",
    "  return toks\n",
    "\n",
    "count_transformer_tokenizer = CountVectorizer(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5JkaG856ebP3"
   },
   "outputs": [],
   "source": [
    "transformed_tweets_tokenizer = count_transformer_tokenizer.fit_transform(df[\"tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tSt8rBZMeZ86"
   },
   "outputs": [],
   "source": [
    "print(len(count_transformer_tokenizer.get_feature_names()))\n",
    "print(sorted(count_transformer_tokenizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WeNbfXRze_ql"
   },
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('count', count_transformer_tokenizer, \"tweet\")]) # importante definir las columnas sobre las cuales se aplica\n",
    "\n",
    "rf = Pipeline(steps=[('preprocesor_tokenizer', preprocessor),\n",
    "                      ('classifier', LogisticRegression())])   \n",
    "\n",
    "rf.fit(X_train,y_train)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvOY8S0Fh-tf"
   },
   "source": [
    "Nota. El ``tokenizer`` y el ``preprocessor`` pueden combinarse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SN5jF2Ec1KX_"
   },
   "source": [
    "### Opción 2: Creamos nuestro Transformer de cero\n",
    "\n",
    "En la opción anterior agregamos nuestro procesamiento al ``Vectorizer`` que ya existe. En esta, vamos a crear nuestro ``Vectorizer`` de cero. Este caso nos permite agregar más procesamiento sin estar atados al agregado de comportamiento al ``tokenizer`` o al ``preprocessor``.\n",
    "\n",
    "Si bien podemos agregar comportamiento más complejo, vamos a ejemplificar con el mismo comportamiento que le dimos al ``tokenizer``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJNNVVTNzM9B"
   },
   "source": [
    "Para esto, debemos crear una nueva clase que extienda los ``Vectorizers`` para el comportamiento que nosotros queremos. En este sentido, tenemos que implementar algunos métodos:\n",
    "* ``fit``. Preparar el modelo interno de nuestro ``Transformer``. Puede que no necesiten hacer nada. En este caso, vamos a hacer un ``fit`` de nuestro ``Vectorizer`` interno.\n",
    "* ``inverse_transform``. Cuál es el resultado de deshacer la transformación? En este caso, no va a ser posible, con lo que retornamos una lista vacía.\n",
    "* ``tranform``. El procesamiento propiamente dicho que queremos aplicar. \n",
    "\n",
    "La complicación que tiene esta implementación es el tipo de datos que se esperan que se retornen. En principio, la salida de esto va directo al modelo a entrenar, por lo que tenemos que retornar aquello que espera dicho ``Transformer``: una representación matricial de nuestros datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GblhYPQY3Pcy"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZ3teD-A3D2b"
   },
   "outputs": [],
   "source": [
    "class PartOfSpeech(BaseEstimator, TransformerMixin): # tweet transformado en su part of speech\n",
    "  def __init__(self, stopwords = None, punct=None, lower=True, strip=True):\n",
    "    self.lower = lower\n",
    "    self.strip      = strip\n",
    "    self.stopwords  = stopwords \n",
    "    self.punct      = r'[!?.,()\\\":$]'\n",
    "    self.nlp = spacy.load('en_core_web_sm')\n",
    "    self.counter = CountVectorizer()\n",
    "\n",
    "  def fit(self,X,y=None):\n",
    "    self.counter.fit([' '.join(self.process(str(doc))) for doc in X.values]) # hacemos el fit de nuestro vectorizer interno\n",
    "    return self\n",
    "\n",
    "  def inverse_transform(self, X):\n",
    "    return []\n",
    "  \n",
    "  def transform(self,X): # acá tenemos que ser cuidadosos. Recordemos que la salida de esto en principio va directo al modelo a entrenar, es por eso que tenemos que retornar\n",
    "    return self.counter.transform(raw_documents=[' '.join(self.process(str(doc))) for doc in X.values])\n",
    "\n",
    "  def process(self,doc):\n",
    "    proc = []\n",
    "    doc = re.sub(self.punct, '', doc)\n",
    "    for token in self.nlp(doc):\n",
    "      proc.append(token.tag_)\n",
    "    return proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcFlY3I6z43c"
   },
   "source": [
    "Finalmente, vamos a ver que se puede utilizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NvChJeaGz77Y"
   },
   "outputs": [],
   "source": [
    "pos_processing = PartOfSpeech()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('pos', pos_processing, \"tweet\")]) # importante definir las columnas sobre las cuales se aplica\n",
    "\n",
    "rf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression())])\n",
    "\n",
    "rf.fit(X_train,y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qq_INwZN9-NU"
   },
   "source": [
    "Vamos a realizar la predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8rc6spv9blY"
   },
   "outputs": [],
   "source": [
    "print(rf.predict(test_set.drop('class',axis=1)))\n",
    "\n",
    "print(test_set['class'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DUIA NLP (B) - Pipelines _ Transformers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
